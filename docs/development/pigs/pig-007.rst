.. include:: ../../references.txt

.. _pig-007:

*********************************************
PIG 7 - Datasets and Joint Likelihood Fitting
*********************************************

* Author: Axel Donath & Regis Terrier
* Created: Nov 26, 2018
* Accepted:
* Status:
* Discussion:


Introduction
============

One of the most important features of Gammapy v1.0 will be the modeling and fitting
of gamma-ray data in 1D, 2D, 3D and combined joint-likelihood analysis scenarios.
The joint-likelihood analysis scenario includes combining 1D, 2D and 3D analyses,
stacked fitting of multiple observations, joint-fitting of multiple observations,
time intervals or event classes and combining data from different IACTs as well
as combining IACT with Fermi-LAT or HAWC data. In this PIG we propose the introduction
of a generalized modeling and fitting API that allows users to solve the
aforementioned use cases with Gammapy. The proposal aims for a fast prototype
implementation for Gammapy v0.11. The final design for Gammapy v1.0 is expected
to change in 2019, based on the experience from the prototype, new developments
in CTA and also studying the design of existing modeling and fitting packages.



Status Quo
==========

Data preparation
----------------
To prepare the gamma-ray data for the modeling and fitting step there are currently
two classes in Gammapy:

**`MapMaker`**: class to compute stacked counts, background and exposure maps
from a list of observations, given a fixed map geometry and offset cut. The
result is a dictionary of `Map` objects.

**`SpectrumExtraction`**: class to compute vectors of counts, background and
effective area data for a list of observations. The results are bundled into
a `SpectrumObservationList` object, containing a list of individual `SpectrumObservation`
objects.

Fitting
-------
Currently there are three classes in Gammapy to perform fitting:

**`MapFit`**: class to perform a binned likelihood fit on map with spatial and energy
axes. It takes a sky model, exposure, a background model and one set of IRFs. It does
not support fitting of the background model, it does not support IRFs per model
component and it does not support joint-likelihood fitting.

**`FluxPointFit`**: class to fit spectral models to flux points. It does not support
joint likelihood fitting.

**`SpectrumFit`**: class to perform a spectral joint-likelihood fit of multiple
observations.

In a first clean-up step a `Fit` base class was introduced, from which all the
class inherit from. This `Fit` class implements a uniform interface to various
fitting backends such as sherpa, iminuit and scipy.optimize. It also implements
a common subset of helper methods such as error estimation and computation of
likelihood profiles as well as likelihood contours.

Models and Model Evaluationin extra
---------------------------
Furthermore there are the following related classes:

**`MapEvaluator`**: class to bundle a sky model, exposure, background and IRFs.
It has the responsibility of computing applying the IRFs and adding the background
model to compute the total number of predicted counts. It uses zero order integration,
by mutipliying with the energy bin width. It does not support IRFs per model component.

**`CountsPredictor`** class to bundle a spectral model, effective area, livetime
and energy dispersion. It computes the predicted number of signal (without background)
counts. It uses `SpectralModel.integrate()` to compute the flux.

**`SkyModels`**: class that represents a list of `SkyModel` components. It is 
evaluted with a Python loop over model components. It does not use bounding boxes
for the evaluation.

**`CompoundSkyModel`**: class that represents an arbitrary arithmetic combination of
two `SkyModel` objects. The "+" operator is implemented by creating a `CompoundSkyModel`.


Proposal
========


Modeling and Fitting
--------------------

As a first step for modeling and fitting the prepared data, the user chooses a
map geometry and an offset-cut. If the user wants to perform a stacked analysis,
the `MapMaker.stack(offset_max=)` method must be run first.

**`MapDataset`** analysis session object, that "orchestrates" the setup and I/O
of the precomputed observation maps, the counts map as well as the model. It holds
the repsonsibility of computing the counts, predicted number of counts as well as
the likelihood. In the `.setup()` method, the following preparation steps are
executed:

* Compute the counts map for the event lists and the given map geometry
* Reproject the background maps on the given map geometry and create an instance
of `BackgroundModel`
* Setup a list of `ModelEvaluator` or `IRFSkyModel` instances for every component
in the `SkyModels` object. For this the corresponding PSF and EDISP is extracted from
the given `PSFMap` and `EDispMap`, based on the position of the model component.
Then cutout a part of  the counts map geometry, based on the position of
the model component and a ROI radius ("bounding box") it defines. Optionally the
cut-out geometry is upsampled in space and energy (for energy the user might
define an array of true energy in the `.setup()` call). The exposure map is
reprojected on this geometry.

The proposed workflow is as following:

.. code::

    ome = MapMaker(geom_coarse)
    ome.run(observations, save_folder="observations/")

    datasets = Datasets.read("observations/")
    datasets.setup(geom=WcsGeom(), model=SkyModel(), offset_max=2.5 * u.deg, stack=False)

    fit = Fit(datasets.likelihood)
    fit.optimize()


**`IRFSkyModel`** or **`ModelEvaluator`**: a "forward folded" model, that applies
IRFs to a `SkyModel` instance and returns an integrated quantity corresponding
to predicted counts. In the current proposal it can only be evaluated on a fixed grid,
which passed on initialization given by the exposure map. Later the evaluation on arbitrary
grids could be introduced. In addition it takes reduced PSF and EDISP objects.
In functionality it corresponds to the current `MapEvaluator`, but with the
model parameters attached. Additional "hidden" IRF parameters e.g. to propagate
systematics could be optionally introduced later. This class will replace the current
`MapEvaluator`. The `IRFSkyModel` can be initialized with a custom geometry, where
the sky model is evaluated on.


**`Datasets`**: join a list of `MapDataset` objects and compute the joined likelihood
across those.

Optional Classes
----------------
The following optional classes could be introduced to distribute the responsibilities
of the `MapDataset` accordingly:

**`NPredModel`**: takes a list of instances of `IRFSkyModel` and / or `BackgroundModel`
and joins the parameters lists and sums up the contributions from all model components
in the list.


Use cases
=========
With the proposed structure the following use cases can be addressed:

1. Simple Stacked Fit
---------------------

.. code::

    # coarse spatial binning for data preparation
    geom = WcsGeom(binsz=0.2 * u.deg)
    ome = ObservationMapEstimator(geom=geom, offset_max=2.5 * u.deg)

    ome.run(observations, outfolder="")
    # or
    ome.run(observations, outfolder="", stack=True)

    maps_stacked = ome.stack(infolder="", offset_max=2.5 * u.deg)

    dataset = MapDataset.read()

    sky_model = SkyModel()
    geom_analysis = WcsGeom(binsz=0.02 * u.deg)
    dataset.setup(geom=geom_analysis, model=sky_model)

    fit = Fit(dataset)
    fit.optimize()


2. Joint Fit across Multiple Observations
-----------------------------------------

.. code::

    # coarse spatial binning for data preparation
    geom = WcsGeom(binsz=0.2 * u.deg)
    ome = ObservationMapEstimator(geom=geom, offset_max=2.5 * u.deg)

    ome.run(observations, outfolder="")
    # or
    ome.run(observations, outfolder="", stack=True)

    maps_stacked = ome.stack(infolder="", offset_max=2.5 * u.deg)

    dataset = MapDataset.read()

    sky_model = SkyModel()
    geom_analysis = WcsGeom(binsz=0.02 * u.deg)
    dataset.setup(geom=geom_analysis, model=sky_model)

    fit = Fit(dataset.likelihood)
    fit.optimize()


3. Joint Likelihood with Fermi
------------------------------

.. code::

    sky_model = SkyModel()
    model = IRFSkyModel(exposure_map, psf, edisp, sky_model)

    diffuse_model = SkyCubeDiffuse.read()
    model_fermi = IRFSkyModel(exposure_map_fermi, psf_fermi, edisp=None, sky_model + diffuse_model)

    background = BackgroundModel(background_map)
    npred = NPredModel([model, background])
    npred_fermi = NPredModel([model_fermi])

    like = CashLikelihood(counts_map_2, npred)
    like_fermi = CashLikelihood(counts_fermi, npred_fermi)

    joint_like = JointLikelihood([like, like_fermi])

    fit = Fit(joint_like)
    fit.optimize()



5. Joint Likelihood with 3D and Flux Points
-------------------------------------------

.. code::

    spectral_model = SpectralModel()
    spatial_model = SpatialModel()

    sky_model = SkyModel(spectral_model, spatial_model)

    dataset = MapDataset.read("")
    dataset.setup()

    fp = FluxPoints.read("")
    dataset_fp = FluxPointsDataset(fp, spectral_model, likelihood="chi2")

    datasets = Datasets([dataset, dataset_fp])

    fit = Fit(datasets.likelihood)
    fit.optimize()




List of Pull Requests
=====================

This is proposal for a list of pull requests implementing the proposed changes.
The following PRs could be implemented independently of each other:

The following PRs should be implemented in order:

- Implement `MapDataset` class
- Implement `Datasets` class
- Implement `FluxPointsDataset` class
- Implement `Datasets` support for the `Fit` class
- Rename `SpectrumObservation` to `SpectrumDataset`
- Remove `MapFit` and `FluxPointFit` classes
- Implement `MapDatasetEstimator` 


Open Issues
===========
In the current design, the geometries used for the evaluation of the model components
are pre-computed in the `MapDataset.setup()` call. If the position of the model
component shifts strongly in the fit, the model component can be outside it's
initial, cached geometry. To solve this problem a mechanism to check the validity
of cached geometries should be added. Short-term the issue can be solved by
defining large enough bounding boxes.



Spectral Analysis
=================
This PIG does not address the question of restructuring the spectral analysis code.
Still we would like to give a short outlook, how it could be integrated in the
proposed fitting framework. The current implementation defines the following classes:

**`SpectrumFit`**: class to perform a spectral joint-likelihood fit of multiple
observations.

**`SpectrumExtraction`**: class to compute vectors of counts, background and
effective area for a list of observations. The results are bundled into
a `SpectrumObservationList` object, containing a list of `SpectrumObservation`
objects.

**`CountsPredictor`** class to bundle a spectral model, effective area, livetime
and energy dispersion. It computes the predicted number of signal (without background)
counts.

The `SpectrumObservation` object could be replaced by a `SpectrumDataset`, which
bundles events, background and IRFs into a single object. Multiple `SpectrumDataset`
objects could be in a `Datasets` object. The need for `SpectrumObservationList`
is unclear. The `SpectrumFit` object could be fully replaced by `Fit`. The `CountsPredictor`
could be replaced by `IRFSpectralModel` or `SpectraModelEvaluator`.

Long term there is also the option to fully replace the spectral analysis by introducing
`RegionGeom` and `RegionNDMap` objects (see https://github.com/gammapy/gammapy/issues/1805).




Alternatives
============

Extend existing `MapEvaluator` and introduce `Dataset` classes
--------------------------------------------------------------
Alternatively the `BackgroundModel` could be implemented deriving from `SkyModel`
and would be defined as part of the sky model like so:

.. code::

    sky_model = SkyModel()
    backround = BackgroundModel()
    total_model = sky_model + background
    assert isinstance(total_model, SkyModels)

The existing `MapEvaluator` must be changed to evaluate the components of
`SkyModels` one by one, check the type of the component add apply IRFs for
the component accordingly. To handle spatially varying IRFs, the `MapEvaluator`
has to be extended to take psf and edisp maps on initialization. For every model
component the look up for the corresponding psf and edisp happens within the
`MapEvaluator` and should be cashed.

To join the data, model and IRFs, a `Dataset` object could be defined, which takes
the counts map, model and reduced IRFs maps and offers builtin likelihood functions.
Internally it sets up the `MapEvaluator` object and computes the likelihood.
For joint-likelihood analyses many instances of the `Dataset` are created and
joined in a `JointDataset` or `Datasets` object, which is passed to the `Fit` object.

1. Simple Stacked Fit
---------------------

.. code::

    sky_model = SkyModel()
    background = BackgroundModel(background_map)

    total_model = sky_model + background

    evaluator = MapEvaluator(exposure_map, psf_map, edisp_map, total_model)

    dataset = MapDataset(counts_map, evaluator, likelihood="cash")

    fit = Fit(dataset)
    fit.optimize()


2. Joint Fit across Multiple Observations
-----------------------------------------

.. code::

    sky_model = SkyModel()

    background_1 = BackgroundModel(background_map_1)
    background_2 = BackgroundModel(background_map_2)

    total_model_1 = sky_model + background_1
    total_model_2 = sky_model + background_2

    evaluator_1 = MapEvaluator(exposure_map_1, psf_map_1, edisp_map_1, total_model_1)
    evaluator_2 = MapEvaluator(exposure_map_2, psf_map_2, edisp_map_2, total_model_2)

    dataset_1 = MapDataset(counts_map_1, evaluator_1, likelihood="cash")
    dataset_2 = MapDataset(counts_map_2, evaluator_2, likelihood="cash")

    datasets = Datasets([dataset_1, dataset_2])
    fit = Fit(datasets)
    fit.optimize()


3. Joint Likelihood with Fermi
------------------------------

.. code::

    sky_model = SkyModel()
    diffuse_model = SkyCubeDiffuse.read()

    background = BackgroundModel(background_map)

    total_model = sky_model + background
    total_model_fermi = sky_model + diffuse_model

    evaluator = MapEvaluator(exposure_map, psf_map, edisp_map, total_model)
    evaluator_fermi = MapEvaluator(exposure_fermi, psf_fermi, total_model_fermi)

    dataset = MapDataset(counts_map, evaluator, likelihood="cash")

    dataset_fermi = MapDataset(counts_fermi, evaluator_fermi, likelihood="cash")

    datasets = Datasets([dataset, dataset_fermi])
    fit = Fit(datasets)
    fit.optimize()


In this scenario the need for "binned models" such as `IRFSkyModel` and `NPredModel`
is avoided by making the background part of the definiton of the sky model and
giving all the responsibility for the correct model evaluation to the `MapEvaluator`.
In addition the caching of models must be implemented. This seems onceptionally not as
clean as the introdcution of the `IRFSkyModel` and `NPredModel`, which really
reflects the structure of the total model in the API. Possibly the `MapEvaluator`
should be renamed to `ModelEvaluator`. In this solution the `Dataset` object is
mandatory. In the solution described above the `Dataset` object could be introduced
optionally as a convenience to the user with the same API.


Other packages
--------------
To decide on a final design it would help to read and understand the implementations
of exiting joint-likelihood analyses frameworks:

* Gammalib (http://gammalib.sourceforge.net/users/user_manual/) has
the same focus as Gammapy.

* Fermipy https://fermipy.readthedocs.io/en/latest/, only supports a "stacked"
analysis and does not have the concept of mutiple observations.

* pointlike (https://nbviewer.jupyter.org/github/tburnett/Fermi-LAT/blob/master/pointlike_document/index.ipynb)

* 3ML (https://threeml.readthedocs.io/en/latest/) uses a plugin system to interface
to the existing mission frameworks.

* Datastack http://cxc.harvard.edu/contrib/datastack/ adds support for joint fits
of multiple datasets in Sherpa.




Decision
========



.. _gammapy: https://github.com/gammapy/gammapy
.. _gammapy-web: https://github.com/gammapy/gammapy-webpage